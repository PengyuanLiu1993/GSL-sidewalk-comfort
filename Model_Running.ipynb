{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OYe8RFFSdLS",
        "outputId": "9c1894d3-4817-4743-f1f3-07da36c50943"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: dgl in /usr/local/lib/python3.7/dist-packages (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (1.21.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (2.23.0)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.7/dist-packages (from dgl) (2.6.3)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (5.9.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (1.7.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from dgl) (4.64.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (3.0.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: jenkspy in /usr/local/lib/python3.7/dist-packages (0.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from jenkspy) (1.21.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install dgl\n",
        "!pip install jenkspy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import random\n",
        "from jenkspy import JenksNaturalBreaks\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import dgl\n",
        "from dgl.nn import SAGEConv\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "import warnings\n",
        "import scipy.stats as stats\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "random.seed(1)"
      ],
      "metadata": {
        "id": "luPZ70G2SiIM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers):\n",
        "        super(MyLSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape (batch, time_step, input_size)\n",
        "        # out shape (batch, time_step, output_size)\n",
        "        # h_n shape (n_layers, batch, hidden_size)\n",
        "        # h_c shape (n_layers, batch, hidden_size)\n",
        "        # 初始化hidden和memory cell参数\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
        "\n",
        "        # forward propagate lstm\n",
        "        out, (h_n, h_c) = self.lstm(x, (h0, c0))\n",
        "\n",
        "        # 选取最后一个时刻的输出\n",
        "        out = out[:, -1, :]\n",
        "        return out"
      ],
      "metadata": {
        "id": "XCFZzcT7SunJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(Dataset):\n",
        "    '''\n",
        "    自定义数据集\n",
        "    '''\n",
        "    def __init__(self, features, labels, graph):\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "        self.graph = graph\n",
        "        # self.altitudes = altitudes\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        feature = self.features[idx]\n",
        "        graph = self.graph[idx]\n",
        "        label = self.labels[idx]\n",
        "        # altitude = self.altitudes[idx]\n",
        "        return feature, graph, label\n",
        "\n",
        "\n",
        "def get_each_person(data):\n",
        "    '''\n",
        "    将所有数据按照85/人划分\n",
        "    :param data:\n",
        "    :return:\n",
        "    '''\n",
        "    i_count = 0\n",
        "    all_person, one_person = [], []\n",
        "    for index, values in data.iterrows():\n",
        "        i_count += 1\n",
        "        if i_count % 85 == 0:\n",
        "            one_person.append(values)\n",
        "            all_person.append(one_person)\n",
        "            one_person = []\n",
        "        else:\n",
        "            one_person.append(values)\n",
        "    return all_person\n",
        "\n",
        "\n",
        "def get_label_class(values):\n",
        "    '''\n",
        "    归一化后的标签离散处理\n",
        "    :param values: 归一化后的标签值\n",
        "    :return:\n",
        "    '''\n",
        "\n",
        "    # return int(values[0]*9)\n",
        "\n",
        "    bins = [0.5, 0.6, 1.1]  # 暂时划分为三个等级\n",
        "    for i in range(len(bins)):\n",
        "        if bins[i] > values[0]:\n",
        "            return i\n",
        "\n",
        "\n",
        "def get_graph(graph):\n",
        "    '''\n",
        "    构图\n",
        "    :param graph: 矩阵14*14\n",
        "    :return:\n",
        "    '''\n",
        "    index = [[], []]\n",
        "    i = 0\n",
        "    for line in graph:\n",
        "        j = 0\n",
        "        for node in line:\n",
        "            if i == j:\n",
        "                break\n",
        "            if node == 1:\n",
        "                index[0].append(i)\n",
        "                index[1].append(j)\n",
        "            j+=1\n",
        "        i += 1\n",
        "    g = dgl.graph((index[0], index[1]))\n",
        "    g = dgl.add_self_loop(g)\n",
        "    return g\n",
        "\n",
        "\n",
        "def get_altitude_gap(data_edm):\n",
        "    altitude = list(data_edm['dem'].values)\n",
        "    gaps = [altitude[-1]-altitude[0]]\n",
        "    for i in range(len(altitude)-1):\n",
        "        gaps.append(altitude[i+1]-altitude[i])\n",
        "    zscores = stats.zscore(gaps)\n",
        "    return zscores.astype(np.float32)\n",
        "\n",
        "\n",
        "def read_data(path_fea, path_label, path_graph):\n",
        "    '''\n",
        "    加载原始数据\n",
        "    :param path_fea:\n",
        "    :param path_label:\n",
        "    :param path_graph:\n",
        "    :return:\n",
        "    '''\n",
        "    features, labels, graphs, altitudes = [], [], [], []\n",
        "    data_fea = pd.read_csv(path_fea)\n",
        "    data_label = pd.read_csv(path_label, header=None).T\n",
        "    data_graph = pd.DataFrame(np.load(path_graph).reshape((len(data_label),-1)))\n",
        "    # data_edm = pd.read_csv(path_dem, index_col=0)\n",
        "\n",
        "    per_fea = get_each_person(data_fea)\n",
        "    per_label = get_each_person(data_label)\n",
        "    per_graph = get_each_person(data_graph)\n",
        "    # altitude_gap = get_altitude_gap(data_edm)\n",
        "\n",
        "    time_window_node, time_window_lstm = 5, 5\n",
        "    for p in range(len(per_fea)):\n",
        "        insert_0 = per_fea[p][0]\n",
        "        for _ in range(time_window_node):\n",
        "            per_fea[p].insert(0, insert_0)\n",
        "\n",
        "        data_label = pd.DataFrame(per_label[p])\n",
        "        data_label_norm = (data_label - data_label.min()) / (data_label.max() - data_label.min())\n",
        "\n",
        "        # data_label_norm = pd.qcut(x=data_label_norm[0], q=3, labels=range(0,3), duplicates=\"drop\")\n",
        "        jnb =JenksNaturalBreaks(3)\n",
        "        jnb.fit(list(data_label_norm[0].values))\n",
        "        data_label_norm = jnb.labels_\n",
        "        data_fea = pd.DataFrame(per_fea[p])\n",
        "        data_graph = pd.DataFrame(per_graph[p])\n",
        "\n",
        "        lstm_fea, graph_ = [], []\n",
        "        for i in range(len(data_fea)-time_window_node):\n",
        "            fea = data_fea.iloc[i:i+time_window_node].values\n",
        "            lstm_fea.append(fea.astype(np.float32))\n",
        "\n",
        "        for i in range(len(lstm_fea)-time_window_lstm):\n",
        "            features.append(np.array(lstm_fea[i:i+time_window_lstm]).astype(np.float32))\n",
        "            graphs.append(data_graph.iloc[i:i+time_window_lstm].values.reshape(time_window_lstm, 14,-1))\n",
        "            # altitudes.append(altitude_gap[i:i + time_window_lstm])\n",
        "            # class_ = get_label_class(data_label_norm.iloc[i+time_window_lstm])\n",
        "            class_ = data_label_norm[i+time_window_lstm]\n",
        "            labels.append(class_)\n",
        "\n",
        "    # features, labels, graphs, altitudes = shuffle(features, labels, graphs, altitudes)\n",
        "    features, labels, graphs = shuffle(features, labels, graphs)\n",
        "\n",
        "    return features, labels, graphs\n",
        "\n",
        "\n",
        "def shuffle(features, labels, graphs):\n",
        "    '''\n",
        "    打乱顺序\n",
        "    :param features:\n",
        "    :param labels:\n",
        "    :param graphs:\n",
        "    :return:\n",
        "    '''\n",
        "    new_features, new_labels, new_graphs = [], [], []\n",
        "    index = [i for i in range(len(features))]\n",
        "    random.shuffle(index)\n",
        "    for i in index:\n",
        "        new_features.append(features[i])\n",
        "        new_labels.append(labels[i])\n",
        "        new_graphs.append(graphs[i])\n",
        "        # new_altitudes.append(altitudes[i])\n",
        "\n",
        "    return new_features, new_labels, new_graphs\n",
        "\n",
        "\n",
        "\n",
        "class My_model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(My_model, self).__init__()\n",
        "        self.graph_embedding = SAGEConv(5, 128, 'pool')\n",
        "        self.flat = nn.Linear(128*14, 128)\n",
        "        self.lstm = MyLSTM(input_size=128, hidden_size=128, num_layers=1)\n",
        "        self.classify = nn.Linear(128, 3)\n",
        "        # self.altitude_emd = nn.Linear(8, 128)\n",
        "\n",
        "        # self.emb = nn.Linear(12*5, 128)\n",
        "\n",
        "        # self.ac = nn.Sigmoid()\n",
        "        self.softmax = nn.Softmax()\n",
        "\n",
        "    def forward(self, features, graph):\n",
        "        features = features.permute(0,1,3,2)\n",
        "        batch, time_step, node, channel = features.shape\n",
        "\n",
        "        # cov_fea_tensor = self.emb(features.reshape(batch, time_step, -1))\n",
        "\n",
        "        cov_fea = []\n",
        "        for i in range(batch):\n",
        "            lstm = []\n",
        "            for j in range(time_step):\n",
        "                fea = features[i, j, :, :]\n",
        "                g = get_graph(graph[i][j])\n",
        "                graph_cov = self.graph_embedding(g, fea).view(-1)\n",
        "                fea_flat = self.flat(graph_cov)\n",
        "                lstm.append(fea_flat)\n",
        "            lstm = torch.stack(lstm, dim=0)\n",
        "            cov_fea.append(lstm)\n",
        "        cov_fea_tensor = torch.stack(cov_fea, dim=0)\n",
        "\n",
        "        lstm_fea = self.lstm(cov_fea_tensor)\n",
        "        # lstm_fea = self.ac(lstm_fea)\n",
        "\n",
        "        # altitude_emd = self.altitude_emd(altitude)\n",
        "        # concat = torch.cat([altitude_emd,lstm_fea], dim=1)\n",
        "        y_class = self.classify(lstm_fea)\n",
        "        y_class_softmax = self.softmax(y_class)\n",
        "        return y_class\n",
        "\n",
        "\n",
        "\n",
        "def val_model(model, val_dataloader):\n",
        "    y_truths, y_preds, val_loss = [], [], []\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        for batch in val_dataloader:\n",
        "            y = batch[2]\n",
        "            y_pre = model.forward(batch[0], batch[1])\n",
        "\n",
        "            loss = loss_fun(y_pre, y)\n",
        "            val_loss.append(loss.item())\n",
        "            pred_i = y_pre.data.max(1, keepdim=True)[1]\n",
        "            y_truths.append(y.numpy()[0])\n",
        "            y_preds.append(pred_i.numpy()[0][0])\n",
        "    val_loss = np.mean(val_loss)\n",
        "    acc = metrics.accuracy_score(y_truths, y_preds)\n",
        "    return acc, val_loss\n",
        "\n",
        "\n",
        "def train_model(model, loss_fun, optimizer, train_dataloader, val_dataloader, epochs):\n",
        "    best_epoch = 1\n",
        "    for epo in range(epochs):\n",
        "        losses = []\n",
        "        for batch in train_dataloader:\n",
        "            y = batch[2]\n",
        "            y_dot = model.forward(batch[0], batch[1])\n",
        "            loss = loss_fun(y_dot, y)\n",
        "            losses.append(loss.item())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        train_loss = np.mean(losses)\n",
        "        val_acc, val_loss = val_model(model, val_dataloader)\n",
        "\n",
        "        if val_acc < best_epoch:\n",
        "            best_epoch = val_acc\n",
        "            torch.save(model.state_dict(), 'epoch_best.pkl')\n",
        "\n",
        "        print('Epochs: %s/%s  Train loss: %.6f  Val loss: %.6f verification accuracy： %.6f' % (epo, epochs, train_loss, val_loss, val_acc))\n",
        "\n",
        "    print(\"Train finished !\")\n",
        "    return model\n",
        "\n",
        "\n",
        "def test_model(model, test_dataloader):\n",
        "    y_truths, y_preds = [], []\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        for batch in test_dataloader:\n",
        "            y = batch[2]\n",
        "            y_pre = model.forward(batch[0], batch[1])\n",
        "\n",
        "            pred_i = y_pre.data.max(1, keepdim=True)[1]\n",
        "            y_truths.append(y.numpy()[0])\n",
        "            y_preds.append(pred_i.numpy()[0][0])\n",
        "    acc = metrics.accuracy_score(y_truths, y_preds)\n",
        "    confusion_matrix = metrics.confusion_matrix(y_truths, y_preds)\n",
        "    print(acc)\n",
        "    print(confusion_matrix)\n",
        "    return confusion_matrix\n",
        "\n"
      ],
      "metadata": {
        "id": "AS0gBkhUSvZU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    path_fea = 'zscore_new_23-08-v2.csv'\n",
        "    path_label = 'label-v2.csv'\n",
        "    path_graph = 'spatial_interactive_graphs_v3.npy'\n",
        "    # path_dem = 'dem.csv'\n",
        "\n",
        "    features, labels, graphs = read_data(path_fea, path_label, path_graph)\n",
        "    # for i in features:\n",
        "    #   print(i.shape)\n",
        "    # df_labels=pd.DataFrame(labels)\n",
        "    # df_labels.plot.density()\n",
        "    features=stats.zscore(features)\n",
        "\n",
        "    epochs = 100\n",
        "    batch_size = 128\n",
        "    lr = 0.0001\n",
        "\n",
        "    train_per = 0.7\n",
        "    val_per = 0.1\n",
        "    train_num = int(len(features)*train_per)\n",
        "    val_num = int(len(features)*val_per)\n",
        "\n",
        "    dataset_train = MyDataset(features[:train_num], labels[:train_num], graphs[:train_num])\n",
        "    # dataset_val = MyDataset(features[train_num:train_num+val_num], labels[train_num:train_num+val_num], graphs[train_num:train_num+val_num])\n",
        "    dataset_test = MyDataset(features[train_num:], labels[train_num:], graphs[train_num:])\n",
        "\n",
        "    train_dataloader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
        "    val_dataloader = DataLoader(dataset_test)\n",
        "    test_dataloader = DataLoader(dataset_test)\n",
        "\n",
        "    model = My_model()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)  # config.learning_rate\n",
        "    loss_fun = nn.CrossEntropyLoss()\n",
        "\n",
        "    train = True\n",
        "    if train:\n",
        "        train_model(model, loss_fun, optimizer, train_dataloader, val_dataloader, epochs)\n",
        "    else:\n",
        "        model.load_state_dict(torch.load('epoch_best.pkl'))\n",
        "\n",
        "    confusion_matrix = test_model(model, test_dataloader)\n",
        "\n",
        "    # sn.heatmap(confusion_matrix, annot=True)\n",
        "    # plt.xlabel('Predict')\n",
        "    # plt.ylabel('Truth')\n",
        "    # plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6tuaxQfnnln",
        "outputId": "2feb4702-966d-4df3-9c24-4e26bbb4ddea"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 0/100  Train loss: 1.048282  Val loss: 1.059211 verification accuracy： 0.438889\n",
            "Epochs: 1/100  Train loss: 1.043524  Val loss: 1.047480 verification accuracy： 0.490278\n",
            "Epochs: 2/100  Train loss: 1.034283  Val loss: 1.007124 verification accuracy： 0.481944\n",
            "Epochs: 3/100  Train loss: 1.045569  Val loss: 1.058080 verification accuracy： 0.483333\n",
            "Epochs: 4/100  Train loss: 1.062031  Val loss: 1.030557 verification accuracy： 0.476389\n",
            "Epochs: 5/100  Train loss: 1.003274  Val loss: 0.988275 verification accuracy： 0.484722\n",
            "Epochs: 6/100  Train loss: 1.004822  Val loss: 1.005825 verification accuracy： 0.470833\n",
            "Epochs: 7/100  Train loss: 0.998543  Val loss: 0.991265 verification accuracy： 0.500000\n",
            "Epochs: 8/100  Train loss: 0.985316  Val loss: 0.984146 verification accuracy： 0.497222\n",
            "Epochs: 9/100  Train loss: 0.962275  Val loss: 0.949244 verification accuracy： 0.536111\n",
            "Epochs: 10/100  Train loss: 0.971470  Val loss: 0.953669 verification accuracy： 0.508333\n",
            "Epochs: 11/100  Train loss: 0.979013  Val loss: 0.935908 verification accuracy： 0.541667\n",
            "Epochs: 12/100  Train loss: 0.941083  Val loss: 0.924275 verification accuracy： 0.547222\n",
            "Epochs: 13/100  Train loss: 0.946251  Val loss: 0.949300 verification accuracy： 0.523611\n",
            "Epochs: 14/100  Train loss: 0.937128  Val loss: 0.976417 verification accuracy： 0.479167\n",
            "Epochs: 15/100  Train loss: 0.950255  Val loss: 0.967961 verification accuracy： 0.488889\n",
            "Epochs: 16/100  Train loss: 0.940385  Val loss: 0.938127 verification accuracy： 0.519444\n",
            "Epochs: 17/100  Train loss: 0.919721  Val loss: 0.923453 verification accuracy： 0.544444\n",
            "Epochs: 18/100  Train loss: 0.916439  Val loss: 0.915822 verification accuracy： 0.537500\n",
            "Epochs: 19/100  Train loss: 0.916698  Val loss: 0.900947 verification accuracy： 0.550000\n",
            "Epochs: 20/100  Train loss: 0.901217  Val loss: 0.886554 verification accuracy： 0.576389\n",
            "Epochs: 21/100  Train loss: 0.887568  Val loss: 0.879281 verification accuracy： 0.594444\n",
            "Epochs: 22/100  Train loss: 0.901625  Val loss: 0.881562 verification accuracy： 0.583333\n",
            "Epochs: 23/100  Train loss: 0.847066  Val loss: 0.882438 verification accuracy： 0.587500\n",
            "Epochs: 24/100  Train loss: 0.864573  Val loss: 0.881171 verification accuracy： 0.593056\n",
            "Epochs: 25/100  Train loss: 0.846220  Val loss: 0.877841 verification accuracy： 0.587500\n",
            "Epochs: 26/100  Train loss: 0.846744  Val loss: 0.874630 verification accuracy： 0.581944\n",
            "Epochs: 27/100  Train loss: 0.833987  Val loss: 0.870709 verification accuracy： 0.561111\n",
            "Epochs: 28/100  Train loss: 0.818108  Val loss: 0.870221 verification accuracy： 0.545833\n",
            "Epochs: 29/100  Train loss: 0.829296  Val loss: 0.867080 verification accuracy： 0.544444\n",
            "Epochs: 30/100  Train loss: 0.824163  Val loss: 0.859649 verification accuracy： 0.563889\n",
            "Epochs: 31/100  Train loss: 0.780355  Val loss: 0.847469 verification accuracy： 0.573611\n",
            "Epochs: 32/100  Train loss: 0.776136  Val loss: 0.835176 verification accuracy： 0.597222\n",
            "Epochs: 33/100  Train loss: 0.779187  Val loss: 0.825521 verification accuracy： 0.602778\n",
            "Epochs: 34/100  Train loss: 0.749739  Val loss: 0.816702 verification accuracy： 0.598611\n",
            "Epochs: 35/100  Train loss: 0.752438  Val loss: 0.809058 verification accuracy： 0.613889\n",
            "Epochs: 36/100  Train loss: 0.741759  Val loss: 0.802788 verification accuracy： 0.616667\n",
            "Epochs: 37/100  Train loss: 0.724967  Val loss: 0.794025 verification accuracy： 0.623611\n",
            "Epochs: 38/100  Train loss: 0.728588  Val loss: 0.786157 verification accuracy： 0.634722\n",
            "Epochs: 39/100  Train loss: 0.698120  Val loss: 0.781752 verification accuracy： 0.626389\n",
            "Epochs: 40/100  Train loss: 0.697589  Val loss: 0.776357 verification accuracy： 0.629167\n",
            "Epochs: 41/100  Train loss: 0.703357  Val loss: 0.773089 verification accuracy： 0.626389\n",
            "Epochs: 42/100  Train loss: 0.700344  Val loss: 0.769161 verification accuracy： 0.633333\n",
            "Epochs: 43/100  Train loss: 0.665247  Val loss: 0.760302 verification accuracy： 0.637500\n",
            "Epochs: 44/100  Train loss: 0.660243  Val loss: 0.752613 verification accuracy： 0.656944\n",
            "Epochs: 45/100  Train loss: 0.647325  Val loss: 0.748461 verification accuracy： 0.654167\n",
            "Epochs: 46/100  Train loss: 0.642837  Val loss: 0.744346 verification accuracy： 0.647222\n",
            "Epochs: 47/100  Train loss: 0.613564  Val loss: 0.736671 verification accuracy： 0.648611\n",
            "Epochs: 48/100  Train loss: 0.605683  Val loss: 0.727445 verification accuracy： 0.654167\n",
            "Epochs: 49/100  Train loss: 0.606255  Val loss: 0.720826 verification accuracy： 0.662500\n",
            "Epochs: 50/100  Train loss: 0.598716  Val loss: 0.718848 verification accuracy： 0.665278\n",
            "Epochs: 51/100  Train loss: 0.582294  Val loss: 0.721984 verification accuracy： 0.658333\n",
            "Epochs: 52/100  Train loss: 0.578866  Val loss: 0.719750 verification accuracy： 0.658333\n",
            "Epochs: 53/100  Train loss: 0.569094  Val loss: 0.711961 verification accuracy： 0.672222\n",
            "Epochs: 54/100  Train loss: 0.538340  Val loss: 0.704537 verification accuracy： 0.673611\n",
            "Epochs: 55/100  Train loss: 0.533449  Val loss: 0.704208 verification accuracy： 0.681944\n",
            "Epochs: 56/100  Train loss: 0.533614  Val loss: 0.700818 verification accuracy： 0.672222\n",
            "Epochs: 57/100  Train loss: 0.527320  Val loss: 0.696045 verification accuracy： 0.676389\n",
            "Epochs: 58/100  Train loss: 0.526178  Val loss: 0.693515 verification accuracy： 0.675000\n",
            "Epochs: 59/100  Train loss: 0.520110  Val loss: 0.692681 verification accuracy： 0.677778\n",
            "Epochs: 60/100  Train loss: 0.511868  Val loss: 0.685696 verification accuracy： 0.686111\n",
            "Epochs: 61/100  Train loss: 0.489671  Val loss: 0.676475 verification accuracy： 0.700000\n",
            "Epochs: 62/100  Train loss: 0.498368  Val loss: 0.668068 verification accuracy： 0.698611\n",
            "Epochs: 63/100  Train loss: 0.475682  Val loss: 0.665374 verification accuracy： 0.705556\n",
            "Epochs: 64/100  Train loss: 0.490258  Val loss: 0.669618 verification accuracy： 0.694444\n",
            "Epochs: 65/100  Train loss: 0.492251  Val loss: 0.665824 verification accuracy： 0.691667\n",
            "Epochs: 66/100  Train loss: 0.464553  Val loss: 0.654941 verification accuracy： 0.706944\n",
            "Epochs: 67/100  Train loss: 0.470905  Val loss: 0.644226 verification accuracy： 0.713889\n",
            "Epochs: 68/100  Train loss: 0.452883  Val loss: 0.643413 verification accuracy： 0.727778\n",
            "Epochs: 69/100  Train loss: 0.477583  Val loss: 0.650075 verification accuracy： 0.713889\n",
            "Epochs: 70/100  Train loss: 0.460498  Val loss: 0.648872 verification accuracy： 0.718056\n",
            "Epochs: 71/100  Train loss: 0.453689  Val loss: 0.635888 verification accuracy： 0.733333\n",
            "Epochs: 72/100  Train loss: 0.432183  Val loss: 0.623048 verification accuracy： 0.737500\n",
            "Epochs: 73/100  Train loss: 0.429449  Val loss: 0.621902 verification accuracy： 0.733333\n",
            "Epochs: 74/100  Train loss: 0.441297  Val loss: 0.631687 verification accuracy： 0.730556\n",
            "Epochs: 75/100  Train loss: 0.457833  Val loss: 0.640138 verification accuracy： 0.718056\n",
            "Epochs: 76/100  Train loss: 0.445599  Val loss: 0.629981 verification accuracy： 0.716667\n",
            "Epochs: 77/100  Train loss: 0.430992  Val loss: 0.604478 verification accuracy： 0.741667\n",
            "Epochs: 78/100  Train loss: 0.418427  Val loss: 0.600400 verification accuracy： 0.748611\n",
            "Epochs: 79/100  Train loss: 0.405561  Val loss: 0.612069 verification accuracy： 0.747222\n",
            "Epochs: 80/100  Train loss: 0.424391  Val loss: 0.634006 verification accuracy： 0.737500\n",
            "Epochs: 81/100  Train loss: 0.460735  Val loss: 0.653662 verification accuracy： 0.720833\n",
            "Epochs: 82/100  Train loss: 0.445150  Val loss: 0.650375 verification accuracy： 0.716667\n",
            "Epochs: 83/100  Train loss: 0.436240  Val loss: 0.634226 verification accuracy： 0.738889\n",
            "Epochs: 84/100  Train loss: 0.405173  Val loss: 0.617442 verification accuracy： 0.752778\n",
            "Epochs: 85/100  Train loss: 0.402503  Val loss: 0.616155 verification accuracy： 0.752778\n",
            "Epochs: 86/100  Train loss: 0.400538  Val loss: 0.629711 verification accuracy： 0.734722\n",
            "Epochs: 87/100  Train loss: 0.420454  Val loss: 0.631828 verification accuracy： 0.740278\n",
            "Epochs: 88/100  Train loss: 0.413040  Val loss: 0.628637 verification accuracy： 0.748611\n",
            "Epochs: 89/100  Train loss: 0.406601  Val loss: 0.618119 verification accuracy： 0.740278\n",
            "Epochs: 90/100  Train loss: 0.396085  Val loss: 0.607425 verification accuracy： 0.754167\n",
            "Epochs: 91/100  Train loss: 0.380656  Val loss: 0.595041 verification accuracy： 0.756944\n",
            "Epochs: 92/100  Train loss: 0.383862  Val loss: 0.605807 verification accuracy： 0.750000\n",
            "Epochs: 93/100  Train loss: 0.391439  Val loss: 0.615453 verification accuracy： 0.745833\n",
            "Epochs: 94/100  Train loss: 0.393306  Val loss: 0.621385 verification accuracy： 0.740278\n",
            "Epochs: 95/100  Train loss: 0.391206  Val loss: 0.625824 verification accuracy： 0.738889\n",
            "Epochs: 96/100  Train loss: 0.389325  Val loss: 0.614228 verification accuracy： 0.741667\n",
            "Epochs: 97/100  Train loss: 0.381482  Val loss: 0.602450 verification accuracy： 0.765278\n",
            "Epochs: 98/100  Train loss: 0.366651  Val loss: 0.586222 verification accuracy： 0.769444\n",
            "Epochs: 99/100  Train loss: 0.362413  Val loss: 0.581642 verification accuracy： 0.765278\n",
            "Train finished !\n",
            "0.7652777777777777\n",
            "[[114  53   8]\n",
            " [ 13 266  37]\n",
            " [  6  52 171]]\n"
          ]
        }
      ]
    }
  ]
}
